# ğŸ“˜ README â€“ Etapa 3: Analiza È™i PregÄƒtirea Setului de Date pentru ReÈ›ele Neuronale

**Disciplina:** ReÈ›ele Neuronale  
**InstituÈ›ie:** POLITEHNICA BucureÈ™ti â€“ FIIR  
**Student:** Ionescu David  
**Data:**

---

## Introducere

Acest document descrie activitÄƒÈ›ile realizate Ã®n **Etapa 3**, concentrÃ¢ndu-se pe pregÄƒtirea setului de date pentru sistemul de AnalizÄƒ a Sentimentelor. Deoarece seturile de date standard (ex: IMDB) conÈ›in erori de etichetare È™i nu acoperÄƒ nuanÈ›e lingvistice complexe (sarcasm, opinii concesive), am optat pentru o strategie de **augmentare sinteticÄƒ controlatÄƒ ("Logic Injection")**, combinÃ¢nd date reale cu date generate programatic pentru a forÈ›a modelul sÄƒ Ã®nveÈ›e tipare logice specifice.

---

##  1. Structura Repository-ului Github (versiunea Etapei 3)

project-rn/ â”œâ”€â”€ README.md â”œâ”€â”€ docs/ â”‚ â””â”€â”€ datasets/ # grafice distribuÈ›ie clase â”œâ”€â”€ data/ â”‚ â”œâ”€â”€ raw/ # IMDB Dataset.csv (date brute) â”‚ â”œâ”€â”€ generated/ # Date generate sintetic (sarcasm, logicÄƒ) â”‚ â”œâ”€â”€ processed/ # Date tokenizate È™i curÄƒÈ›ate â”‚ â”œâ”€â”€ train/ # Set de antrenare (85%) â”‚ â”œâ”€â”€ validation/ # Set de validare (15%) â”‚ â””â”€â”€ test/ # Set de testare (inclus Ã®n validare pentru rapiditate) â”œâ”€â”€ src/ â”‚ â”œâ”€â”€ preprocessing/ # Tokenizer, Padding sequences â”‚ â”œâ”€â”€ data_acquisition/ # Scriptul de generare (train.py) â”‚ â””â”€â”€ neural_network/ # Modelul Bi-LSTM + Attention â”œâ”€â”€ config/ â”‚ â””â”€â”€ tokenizer.pkl # DicÈ›ionarul de cuvinte salvat â””â”€â”€ requirements.txt # tensorflow, pandas, numpy, streamlit


---

##  2. Descrierea Setului de Date

### 2.1 Sursa datelor

* **Origine:** HibridÄƒ.
    1. **Baza:** Dataset public IMDB (Kaggle) - recenzii de film reale.
    2. **Augmentare (MajoritarÄƒ):** Generare programaticÄƒ folosind scripturi Python proprii (`src/neural_network/train.py`).
* **Modul de achiziÈ›ie:** â˜‘ Generare programaticÄƒ (Logic Injection) + â˜‘ FiÈ™ier extern (Kaggle).
* **MotivaÈ›ie:** Datele reale nu conÈ›ineau suficiente exemple de sarcasm ("best cure for insomnia") sau structuri complexe ("boring start but great ending"), ducÃ¢nd la erori de context.

### 2.2 Caracteristicile dataset-ului

* **NumÄƒr total de observaÈ›ii:** ~45.000 (variabil Ã®n funcÈ›ie de parametrii de generare).
* **NumÄƒr de caracteristici (features):** 1 (Textul recenziei) -> transformat Ã®n secvenÈ›Äƒ de 200 intregi.
* **Tipuri de date:** â˜‘ Text (NLP) / â˜‘ Numerice (Scoruri sentiment).
* **Format fiÈ™iere:** CSV (pentru stocare) È™i Pandas DataFrame (Ã®n memorie).

### 2.3 Descrierea etichetelor (Target)

| **EtichetÄƒ (Score)** | **SemnificaÈ›ie** | **Exemplu** |
|-------------------|------------------|-------------|
| **0.0** | Negativ | "This movie is a waste of time." / "Best cure for insomnia." |
| **0.5** | Neutru / Average | "It was an average movie, nothing special." |
| **1.0** | Pozitiv | "A masterpiece." / "Boring start but amazing ending." |

---

##  3. Analiza Exploratorie a Datelor (EDA)

### 3.1 Statistici descriptive

* **Lungimea medie a recenziilor:** VariabilÄƒ (de la 3 cuvinte la 500+ cuvinte).
* **Vocabular:** Am limitat vocabularul la cele mai frecvente **15.000 de cuvinte** pentru a elimina zgomotul (nume proprii rare, greÈ™eli de tipar).
* **DistribuÈ›ia claselor:**
    * IniÈ›ial (Kaggle): Puternic polarizat (doar Pozitiv/Negativ).
    * Final (Hibrid): Echilibrat artificial pentru a include clasa NeutrÄƒ È™i cazurile de Sarcasm ("Edge cases").

### 3.2 Probleme identificate Ã®n datele brute (Raw Data)

* **Lipsa Zonei Neutre:** Dataset-ul IMDB forÈ›eazÄƒ recenziile de nota 5 sau 6 Ã®n categoriile "Negativ" sau "Pozitiv", creÃ¢nd confuzie modelului.
* **Orbire la Context:** Cuvintele "Best", "Great", "Cure" apar frecvent Ã®n recenzii negative sarcastice, dar statistic sunt asociate cu clasa pozitivÄƒ.
* **Contaminare:** Expresii precum "Not bad" erau adesea etichetate greÈ™it Ã®n dataset-urile automate.

---

##  4. Preprocesarea Datelor

### 4.1 CurÄƒÈ›area È™i Generarea Datelor (Data Cleaning & Generation)

Ãn loc sÄƒ curÄƒÈ›Äƒm manual datele eronate, am aplicat o strategie de **Generare ControlatÄƒ**:
* **Happy End Scenarios:** Am generat fraze de tip "Start RÄƒu -> Final Bun" etichetate corect (1.0).
* **Sarcasm Injection:** Am generat mii de exemple de tip "Watch paint dry" etichetate corect (0.0).
* **Tratarea valorilor lipsÄƒ:** Nu existÄƒ (datele sunt generate sau curÄƒÈ›ate la citire).

### 4.2 Transformarea caracteristicilor (NLP Pipeline)

* **Tokenizare:** Transformarea textului Ã®n numere folosind un `Tokenizer` antrenat pe corpus (max_words=15000). Caracterul `<OOV>` este folosit pentru cuvinte necunoscute.
* **Padding:** Uniformizarea secvenÈ›elor la lungimea fixÄƒ de **200 de tokeni** (padding='post', truncating='post') pentru a fi compatibile cu intrarea reÈ›elei LSTM.
* **Encoding:** Etichetele sunt valori float continue (0.0 - 1.0) pentru a permite regresia sentimentului (inclusiv zona gri 0.5).

### 4.3 Structurarea seturilor de date

**ÃmpÄƒrÈ›ire:**
* **Train:** ~85% (PrioritizÄƒm volumul mare pentru a expune modelul la variaÈ›ii de sarcasm).
* **Validation/Test:** ~15% (Folosit pentru monitorizarea `val_loss` È™i Early Stopping).

**Principii respectate:**
* **Data Leakage:** Generarea datelor de test se face separat sau prin `train_test_split` cu seed fix (`random_state=42`) pentru reproductibilitate.

### 4.4 Salvarea rezultatelor

* Tokenizer-ul este salvat Ã®n `config/tokenizer.pkl` pentru a fi folosit identic Ã®n aplicaÈ›ia de inferenÈ›Äƒ (UI).
* Modelul antrenat este salvat Ã®n `models/optimized_model.h5`.

---

##  5. FiÈ™iere Generate Ã®n AceastÄƒ EtapÄƒ

* `src/neural_network/train.py` â€“ Scriptul principal care combinÄƒ generarea datelor cu preprocesarea È™i antrenarea.
* `config/tokenizer.pkl` â€“ Obiectul de preprocesare salvat.
* `data/processed/kaggle_combined.csv` â€“ Subsetul de date reale curÄƒÈ›ate.

---

##  6. Stare EtapÄƒ

- [x] StructurÄƒ repository configuratÄƒ
- [x] Dataset analizat (Identificat lipsa sarcasmului È™i a clasei neutre)
- [x] Date preprocesate (Tokenizare + Padding)
- [x] Date augmentate (Logic Injection pentru sarcasm)
- [x] Seturi train/val generate
- [x] DocumentaÈ›ie actualizatÄƒ

---
